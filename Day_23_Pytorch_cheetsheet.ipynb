{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af1d00c",
   "metadata": {},
   "source": [
    "# PyTorch Intermediate Deep Learning Cheat Sheet\n",
    "\n",
    "**Focus:** Intermediate concepts for Computer Vision and Sequential Models\n",
    "\n",
    "**Contents:**\n",
    "- Custom CNNs & Pretrained Models\n",
    "- Transfer Learning & Data Augmentation\n",
    "- Training Loops & Optimization\n",
    "- RNNs, LSTMs, GRUs\n",
    "- Attention Mechanisms\n",
    "- Model Evaluation & Saving\n",
    "- Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9eacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports for Intermediate PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Computer Vision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f197",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Computer Vision\n",
    "\n",
    "### Custom CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e81565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 7,575,370\n"
     ]
    }
   ],
   "source": [
    "# Custom CNN with Modern Techniques\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: Basic conv layers\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Batch normalization for stability\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2: Deeper features\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3: High-level features\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling for flexible input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),  # Regularization\n",
    "            nn.Linear(256 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = CustomCNN(num_classes=10, dropout_rate=0.3)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63a5db",
   "metadata": {},
   "source": [
    "### Pretrained Models & Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27878b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zeshan\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zeshan\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in feature extractor: 2,565\n",
      "Trainable params in fine-tuned: 8,526,341\n"
     ]
    }
   ],
   "source": [
    "# Transfer Learning with Pretrained Models\n",
    "\n",
    "# 1. Feature Extraction (Freeze backbone)\n",
    "def create_feature_extractor(num_classes, model_name='resnet18'):\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        # Freeze all parameters\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace classifier\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    elif model_name == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace classifier\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 2. Fine-tuning (Unfreeze some layers)\n",
    "def create_fine_tuned_model(num_classes, model_name='resnet18', freeze_layers=True):\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        if freeze_layers:\n",
    "            # Freeze early layers, unfreeze later ones\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'layer4' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier with dropout\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(model.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "feature_extractor = create_feature_extractor(num_classes=5, model_name='resnet18')\n",
    "fine_tuned_model = create_fine_tuned_model(num_classes=5, model_name='resnet18')\n",
    "\n",
    "print(f\"Trainable params in feature extractor: {sum(p.numel() for p in feature_extractor.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Trainable params in fine-tuned: {sum(p.numel() for p in fine_tuned_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3287ee8",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1799fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined. Use: model, history = train_model(model, train_loader, val_loader)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Data Augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Advanced Training Loop with Learning Rate Scheduling\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for regularization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # AdamW with weight decay\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    # Alternative: StepLR for step-wise decay\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradient computation for validation\n",
    "            for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
    "        print(f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        scheduler.step()  # Update learning rate\n",
    "        print()\n",
    "    \n",
    "    return model, (train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "print(\"Training function defined. Use: model, history = train_model(model, train_loader, val_loader)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffe9da",
   "metadata": {},
   "source": [
    "## üîÑ Sequential Models\n",
    "\n",
    "### RNNs, LSTMs, and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3e9e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model parameters: 3,779,330\n",
      "GRU model parameters: 3,187,458\n"
     ]
    }
   ],
   "source": [
    "# Sequential Models: RNN, LSTM, GRU\n",
    "\n",
    "class TextClassifierRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, \n",
    "                 num_classes=2, dropout=0.3, rnn_type='LSTM'):\n",
    "        super(TextClassifierRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # RNN layers\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                              batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, \n",
    "                             batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        else:  # Simple RNN\n",
    "            self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, \n",
    "                             batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Classifier (bidirectional doubles the hidden size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Pack sequences for efficiency (if lengths provided)\n",
    "        if lengths is not None:\n",
    "            embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Unpack if we packed\n",
    "        if lengths is not None:\n",
    "            output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # Use last hidden state (for bidirectional, concatenate both directions)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            # hidden shape: (num_layers * 2, batch_size, hidden_dim)\n",
    "            final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # Last layer, both directions\n",
    "        else:\n",
    "            final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(final_hidden)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "model_lstm = TextClassifierRNN(vocab_size=10000, rnn_type='LSTM')\n",
    "model_gru = TextClassifierRNN(vocab_size=10000, rnn_type='GRU')\n",
    "\n",
    "print(f\"LSTM model parameters: {sum(p.numel() for p in model_lstm.parameters()):,}\")\n",
    "print(f\"GRU model parameters: {sum(p.numel() for p in model_gru.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60147896",
   "metadata": {},
   "source": [
    "### Basic Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7131afbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention model parameters: 2,202,883\n",
      "Output shape: torch.Size([4, 2])\n",
      "Attention weights shape: torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "# Basic Attention Mechanism for Sequence Models\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_classes=2, dropout=0.3):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Bidirectional LSTM\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        # Embedding and LSTM\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = self.attention(lstm_out)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # Normalize across sequence\n",
    "        \n",
    "        # Weighted sum (attention-weighted representation)\n",
    "        attended = torch.sum(lstm_out * attention_weights, dim=1)  # (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(attended)\n",
    "        return output, attention_weights.squeeze(-1)  # Return attention for visualization\n",
    "\n",
    "# Sequence padding utility for batching\n",
    "def collate_sequences(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Get lengths before padding\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(seq) for seq in sequences], \n",
    "        batch_first=True, \n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    return padded_sequences, torch.tensor(labels), lengths\n",
    "\n",
    "# Example usage\n",
    "attention_model = AttentionRNN(vocab_size=10000)\n",
    "print(f\"Attention model parameters: {sum(p.numel() for p in attention_model.parameters()):,}\")\n",
    "\n",
    "# Example forward pass\n",
    "sample_input = torch.randint(1, 100, (4, 20))  # Batch of 4, max length 20\n",
    "output, attention_weights = attention_model(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fb1b6",
   "metadata": {},
   "source": [
    "## üéØ Model Evaluation & Saving\n",
    "\n",
    "### Evaluation Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31eaf17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation Functions\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cuda', return_predictions=False):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if return_predictions:\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    if return_predictions:\n",
    "        return accuracy, avg_loss, all_predictions, all_labels\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Model Saving and Loading\n",
    "def save_model(model, optimizer, epoch, loss, filepath):\n",
    "    \"\"\"Save model with training state\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'model_architecture': str(model),  # Save architecture info\n",
    "    }, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, optimizer, filepath, device='cuda'):\n",
    "    \"\"\"Load model and training state\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return model, optimizer, epoch, loss\n",
    "\n",
    "# Model inference function\n",
    "def predict_single(model, input_tensor, device='cuda', return_probabilities=False):\n",
    "    \"\"\"Make prediction on single input\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        if len(input_tensor.shape) == 3:  # Add batch dimension if needed\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        if return_probabilities:\n",
    "            return predicted.item(), probabilities.cpu().numpy()[0]\n",
    "        return predicted.item()\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8702073",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization Tips\n",
    "\n",
    "### Training Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d29f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance optimization tools defined!\n",
      "\n",
      "Key Tips:\n",
      "- Use mixed precision for faster training on modern GPUs\n",
      "- Implement early stopping to prevent overfitting\n",
      "- Use appropriate learning rate schedules\n",
      "- Apply gradient clipping for RNNs and deep networks\n",
      "- Consider dropout scheduling for better regularization\n"
     ]
    }
   ],
   "source": [
    "# Performance Optimization Techniques\n",
    "\n",
    "# 1. Mixed Precision Training (for faster training on modern GPUs)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_with_mixed_precision(model, train_loader, optimizer, criterion, device='cuda'):\n",
    "    \"\"\"Training loop with mixed precision for faster training\"\"\"\n",
    "    scaler = GradScaler()  # For gradient scaling\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast for forward pass\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Scale loss and backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "# 2. Learning Rate Schedulers for Better Convergence\n",
    "def get_scheduler(optimizer, scheduler_type='cosine', **kwargs):\n",
    "    \"\"\"Get different types of learning rate schedulers\"\"\"\n",
    "    if scheduler_type == 'cosine':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=kwargs.get('T_max', 50))\n",
    "    elif scheduler_type == 'step':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=kwargs.get('step_size', 10), \n",
    "                                       gamma=kwargs.get('gamma', 0.1))\n",
    "    elif scheduler_type == 'plateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                   patience=kwargs.get('patience', 5),\n",
    "                                                   factor=kwargs.get('factor', 0.5))\n",
    "    elif scheduler_type == 'warmup':\n",
    "        return optim.lr_scheduler.OneCycleLR(optimizer, max_lr=kwargs.get('max_lr', 1e-3),\n",
    "                                           steps_per_epoch=kwargs.get('steps_per_epoch', 100),\n",
    "                                           epochs=kwargs.get('epochs', 25))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")\n",
    "\n",
    "# 3. Dropout Schedules and Regularization\n",
    "class DropoutScheduler:\n",
    "    \"\"\"Gradually reduce dropout during training\"\"\"\n",
    "    def __init__(self, model, initial_dropout=0.5, final_dropout=0.1, total_epochs=100):\n",
    "        self.model = model\n",
    "        self.initial_dropout = initial_dropout\n",
    "        self.final_dropout = final_dropout\n",
    "        self.total_epochs = total_epochs\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        # Linear decay\n",
    "        current_dropout = self.initial_dropout - (self.initial_dropout - self.final_dropout) * (epoch / self.total_epochs)\n",
    "        current_dropout = max(current_dropout, self.final_dropout)\n",
    "        \n",
    "        # Update all dropout layers\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = current_dropout\n",
    "\n",
    "# 4. Early Stopping Implementation\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "# 5. Gradient Clipping for Stable Training\n",
    "def train_with_gradient_clipping(model, train_loader, optimizer, criterion, max_norm=1.0):\n",
    "    \"\"\"Training with gradient clipping to prevent exploding gradients\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Performance optimization tools defined!\")\n",
    "print(\"\\nKey Tips:\")\n",
    "print(\"- Use mixed precision for faster training on modern GPUs\")\n",
    "print(\"- Implement early stopping to prevent overfitting\")\n",
    "print(\"- Use appropriate learning rate schedules\")\n",
    "print(\"- Apply gradient clipping for RNNs and deep networks\")\n",
    "print(\"- Consider dropout scheduling for better regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7874d5",
   "metadata": {},
   "source": [
    "### Common Loss Functions for Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ca1e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions reference complete!\n",
      "\n",
      "Choose loss based on task:\n",
      "- Multi-class: CrossEntropyLoss\n",
      "- Binary: BCEWithLogitsLoss\n",
      "- Imbalanced: FocalLoss or weighted CrossEntropyLoss\n",
      "- Regression: MSELoss (sensitive to outliers) or SmoothL1Loss (robust)\n"
     ]
    }
   ],
   "source": [
    "# Common Loss Functions Reference\n",
    "\n",
    "# 1. Classification Tasks\n",
    "classification_losses = {\n",
    "    'CrossEntropy': nn.CrossEntropyLoss(),  # Most common for multi-class\n",
    "    'CrossEntropy_weighted': nn.CrossEntropyLoss(weight=torch.tensor([0.5, 2.0])),  # For imbalanced classes\n",
    "    'CrossEntropy_smoothed': nn.CrossEntropyLoss(label_smoothing=0.1),  # Regularization\n",
    "    'BCEWithLogits': nn.BCEWithLogitsLoss(),  # Binary classification\n",
    "    'Focal': None,  # Custom implementation below\n",
    "}\n",
    "\n",
    "# Focal Loss for imbalanced datasets\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# 2. Regression Tasks\n",
    "regression_losses = {\n",
    "    'MSE': nn.MSELoss(),  # Mean Squared Error\n",
    "    'MAE': nn.L1Loss(),   # Mean Absolute Error\n",
    "    'Huber': nn.SmoothL1Loss(),  # Robust to outliers\n",
    "}\n",
    "\n",
    "# 3. Custom Loss Combinations\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combine multiple losses with weights\"\"\"\n",
    "    def __init__(self, losses, weights):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.losses = losses\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        total_loss = 0\n",
    "        for loss_fn, weight in zip(self.losses, self.weights):\n",
    "            total_loss += weight * loss_fn(outputs, targets)\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "focal_loss = FocalLoss(alpha=1, gamma=2)\n",
    "combined_loss = CombinedLoss(\n",
    "    losses=[nn.CrossEntropyLoss(), focal_loss],\n",
    "    weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "print(\"Loss functions reference complete!\")\n",
    "print(\"\\nChoose loss based on task:\")\n",
    "print(\"- Multi-class: CrossEntropyLoss\")\n",
    "print(\"- Binary: BCEWithLogitsLoss\")\n",
    "print(\"- Imbalanced: FocalLoss or weighted CrossEntropyLoss\")\n",
    "print(\"- Regression: MSELoss (sensitive to outliers) or SmoothL1Loss (robust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5fcbc",
   "metadata": {},
   "source": [
    "## üìù Quick Reference Summary\n",
    "\n",
    "### Key Patterns to Remember\n",
    "\n",
    "**Model Definition:**\n",
    "- Always use `nn.Module` as base class\n",
    "- Use `nn.Sequential` for simple stacks\n",
    "- Add `BatchNorm` after conv layers for stability\n",
    "- Use `nn.Dropout` for regularization\n",
    "\n",
    "**Training Loop:**\n",
    "- Set `model.train()` before training\n",
    "- Set `model.eval()` before evaluation\n",
    "- Use `torch.no_grad()` during validation/inference\n",
    "- Always call `optimizer.zero_grad()` before backward pass\n",
    "\n",
    "**Transfer Learning:**\n",
    "- Freeze early layers: `param.requires_grad = False`\n",
    "- Replace final classifier layer\n",
    "- Use lower learning rates for pretrained layers\n",
    "\n",
    "**Sequential Models:**\n",
    "- Use `pack_padded_sequence` for variable lengths\n",
    "- Apply gradient clipping for RNNs: `clip_grad_norm_`\n",
    "- Bidirectional RNNs double the hidden size\n",
    "\n",
    "**Performance:**\n",
    "- Use mixed precision training with `autocast`\n",
    "- Implement early stopping\n",
    "- Choose appropriate learning rate schedules\n",
    "- Save model state dict, not the entire model\n",
    "\n",
    "**Common Gotchas:**\n",
    "- Always move data to device: `.to(device)`\n",
    "- Use `model.eval()` for inference\n",
    "- Remember to set dropout and batch norm to eval mode\n",
    "- Check tensor shapes frequently during development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af0ad434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ PyTorch Intermediate Cheat Sheet Complete!\n",
      "\n",
      "This cheat sheet covers:\n",
      "- Custom CNN architectures with modern techniques\n",
      "- Transfer learning strategies\n",
      "- Advanced training loops with schedulers\n",
      "- RNN/LSTM/GRU implementations\n",
      "- Basic attention mechanisms\n",
      "- Model evaluation and saving\n",
      "- Performance optimization tips\n",
      "- Common loss functions\n",
      "\n",
      "Ready for intermediate deep learning projects! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Quick Start Examples\n",
    "\n",
    "# 1. Image Classification with Transfer Learning\n",
    "\"\"\"\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\"\"\"\n",
    "\n",
    "# 2. Text Classification with LSTM\n",
    "\"\"\"\n",
    "model = TextClassifierRNN(vocab_size=10000, rnn_type='LSTM')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Don't forget gradient clipping for RNNs!\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\"\"\"\n",
    "\n",
    "# 3. Model Evaluation Template\n",
    "\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Process outputs...\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n‚úÖ PyTorch Intermediate Cheat Sheet Complete!\")\n",
    "print(\"\\nThis cheat sheet covers:\")\n",
    "print(\"- Custom CNN architectures with modern techniques\")\n",
    "print(\"- Transfer learning strategies\")\n",
    "print(\"- Advanced training loops with schedulers\")\n",
    "print(\"- RNN/LSTM/GRU implementations\")\n",
    "print(\"- Basic attention mechanisms\")\n",
    "print(\"- Model evaluation and saving\")\n",
    "print(\"- Performance optimization tips\")\n",
    "print(\"- Common loss functions\")\n",
    "print(\"\\nReady for intermediate deep learning projects! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5506c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "Memory: 8.6 GB\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "Torchvision Version: 0.20.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# Device Setup and Compatibility Check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14034d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üî• Pro Tips:**\n",
    "- Always validate your model architecture with dummy input: `model(torch.randn(1, 3, 224, 224))`\n",
    "- Use `torchsummary` or `torchinfo` to inspect model architecture\n",
    "- Monitor GPU memory usage during training\n",
    "- Save checkpoints frequently for long training runs\n",
    "- Use tensorboard or wandb for experiment tracking\n",
    "- Test data loading pipeline separately before training\n",
    "\n",
    "**üîó Useful Libraries:**\n",
    "- `timm` - Modern computer vision models\n",
    "- `transformers` - For NLP (when ready for advanced topics)\n",
    "- `pytorch-lightning` - Simplified training loops\n",
    "- `torchmetrics` - Additional evaluation metrics\n",
    "\n",
    "**Happy Deep Learning! ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
