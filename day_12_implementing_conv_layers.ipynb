{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2c41fc-68f9-4983-8417-e767750f669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import numpy as np \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset , DataLoader , random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7e36f6-1954-4c0a-92aa-6577b8d1a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571e7d34-26a0-4bb6-b9f0-bef55bdf141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6ac05f-9f56-4090-82a4-97a658bd1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ImageFolder(root='Pet_Images' , transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b5bce7-4866-4dcc-bc1c-a791d010e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size =int( 0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ba1631-9db8-4147-b096-e562e0b543c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset , test_dataset = random_split(full_dataset , [train_size , test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a079c1af-19f7-4910-aa09-faef86d275a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19967"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96eb9634-7bed-4ac2-a46c-104ba4faf32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4992"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72833cdc-36a5-42fe-a29f-17ed8d6ca48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset , batch_size=32 , shuffle=True , pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=32 , shuffle=True , pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c3cd2c4-883b-44f8-bfe9-c2a934aa816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mynn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3 , 32 , kernel_size=(3,3) , padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2) , stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)            \n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b606ec9-09fc-4fe8-9950-13dffe08ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mynn().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d048fcb-9edc-41dd-bb3c-a445be3b3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mynn(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=50176, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5539ed8c-9d2f-4caa-b295-2e6da72aa014",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.01 , weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "732b0030-e413-457c-bdb4-70eedd704e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Loop through all batches of data\n",
    "    for inputs, labels in dataloader:\n",
    "        # Move the data to the selected device (CPU or GPU)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model's weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this batch's loss to the total loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Get the predicted class labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update total number of samples and correct predictions\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return average_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ff7cd7d-8865-450d-b1aa-d2d33327b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zeshan\\anaconda3\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n\u001b[0;32m     17\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m     18\u001b[0m test_accs\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d276f-9a26-4a80-92d3-d3d4250d0121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
